{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TensorFlow and NumPy\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set data type\n",
    "DTYPE='float32'\n",
    "tf.keras.backend.set_floatx(DTYPE) \n",
    "\n",
    "# Set constants\n",
    "e0=1.\n",
    "e1=10.\n",
    "D=1.\n",
    "k=0.55\n",
    "\n",
    "# Define initial condition\n",
    "def fun_u_0(x,u,u_x):\n",
    "    return tf.stack([u_x[:,0]-k*np.sqrt(e1)*u[:,1],u_x[:,0]+k*np.sqrt(e0)*u_x[:,0]-2*k*np.sqrt(e0)],axis=1)\n",
    "    #return np.array([u_x[0]-k*np.sqrt(e1)*u[1],u_x[0]+k*np.sqrt(e0)*u_x[0]-2*k*np.sqrt(e0)])\n",
    "    #return u_x+1j*k*np.sqrt(e0)*u-2j*k*np.sqrt(e0)\n",
    "\n",
    "# Define boundary condition\n",
    "def fun_u_d(x,u,u_x):\n",
    "    return tf.stack([u_x[:,0]+k*np.sqrt(e0)*u[:,1],u_x[:,1]+k*np.sqrt(e0)*u[:,0]-2*k*np.sqrt(e0)],axis=1)\n",
    "    #return np.array([u_x[0]+k*np.sqrt(e0)*u[1],u_x[1]+k*np.sqrt(e0)*u[0]-2*k*np.sqrt(e0)])\n",
    "    #return u_x-1j*k*np.sqrt(e0)*u\n",
    "\n",
    "# Define residual of the PDE\n",
    "def fun_r(x, u, u_x, u_xx):\n",
    "    #return np.array([0.,0.])\n",
    "    return tf.stack([u_xx[:,0]+k*k*e1*u[:,0],u_xx[:,1]+k*k*e1*u[:,1]],axis=1)\n",
    "    #return np.array([u_xx[:,0]+k*k*e1*u[:,0],u_xx[:,1]+k*k*e1*u[:,1]])\n",
    "    #return u_xx+k*k*e1*u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of data points\n",
    "#N_0 = 50\n",
    "N_r = 1000\n",
    "\n",
    "# Set boundary\n",
    "xmin = 0.\n",
    "xmax = D\n",
    "\n",
    "# Lower bounds\n",
    "lb = tf.constant( xmin, dtype=DTYPE) \n",
    "# Upper bounds\n",
    "ub = tf.constant( xmax, dtype=DTYPE)\n",
    "\n",
    "# Set random seed for reproducible results\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# Draw uniform sample points for initial boundary data\n",
    "\n",
    "#x_0 = tf.random.uniform((N_0,1), lb, ub, dtype=DTYPE)\n",
    "\n",
    "# Evaluate intitial condition at x_0\n",
    "#u_0 = fun_u_0()\n",
    "#u_d = fun_u_d()\n",
    "\n",
    "\n",
    "# Draw uniformly sampled collocation points\n",
    "X_r = tf.random.uniform((N_r,1), lb, ub, dtype=DTYPE)\n",
    "X_r = tf.concat([X_r, X_r], axis=1)\n",
    "\n",
    "\n",
    "# Collect boundary and inital data in lists\n",
    "X_data = [0, D]\n",
    "#u_data = [u_0, u_d]\n",
    "x_0=tf.zeros([1,2])\n",
    "x_b=x_0=tf.reshape(tf.stack([ub,ub],axis=0),(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(num_hidden_layers=4, num_neurons_per_layer=20):\n",
    "    # Initialize a feedforward neural network\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # Input is two-dimensional (time + one spatial dimension)\n",
    "    model.add(tf.keras.Input(2))\n",
    "\n",
    "    # Introduce a scaling layer to map input to [lb, ub]\n",
    "    scaling_layer = tf.keras.layers.Lambda(\n",
    "                lambda x: (x - lb)/(ub - lb) ) \n",
    "    model.add(scaling_layer)\n",
    "\n",
    "    # Append hidden layers\n",
    "    for _ in range(num_hidden_layers):\n",
    "        model.add(tf.keras.layers.Dense(num_neurons_per_layer,\n",
    "            activation=tf.keras.activations.get('tanh'),\n",
    "            kernel_initializer='glorot_normal'))\n",
    "\n",
    "    # Output is one-dimensional\n",
    "    model.add(tf.keras.layers.Dense(2))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_r(model,f):\n",
    "    \n",
    "    # A tf.GradientTape is used to compute derivatives in TensorFlow\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # Split t and x to compute partial derivatives\n",
    "        if f==fun_r:\n",
    "            x = X_r\n",
    "        elif f==fun_u_0:\n",
    "            x=x_0\n",
    "        elif f==fun_u_d:\n",
    "            x=x_b\n",
    "        # Variables t and x are watched during tape\n",
    "        # to compute derivatives u_t and u_x\n",
    "        tape.watch(x)\n",
    "\n",
    "        # Determine residual \n",
    "        u = model(x)\n",
    "        # Compute gradient u_x within the GradientTape\n",
    "        # since we need second derivatives\n",
    "        u_x = tape.gradient(u, x)\n",
    "\n",
    "    u_xx = tape.gradient(u_x, x)\n",
    "\n",
    "    del tape\n",
    "    if f == fun_r:\n",
    "        return fun_r( x, u, u_x, u_xx)\n",
    "    elif f == fun_u_0:\n",
    "        print(fun_u_0(x,u,u_x))\n",
    "        return fun_u_0(x,u,u_x)\n",
    "    elif f ==fun_u_d:\n",
    "        return fun_u_d(x,u,u_x)\n",
    "    else:\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def get_u_0(model,X_r):\\n    with tf.GradientTape(persistent=True) as tape:\\n        \\n        tape.watch(X_r)\\n\\n        u = model(tf.stack( X_r, axis=1))\\n\\n        # Compute gradient u_x within the GradientTape\\n        # since we need second derivatives\\n        u_x = tape.gradient(u, X_r)\\n    \\n    return '"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def get_u_0(model,X_r):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        \n",
    "        tape.watch(X_r)\n",
    "\n",
    "        u = model(tf.stack( X_r, axis=1))\n",
    "\n",
    "        # Compute gradient u_x within the GradientTape\n",
    "        # since we need second derivatives\n",
    "        u_x = tape.gradient(u, X_r)\n",
    "    \n",
    "    return '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, X_r, X_data):\n",
    "    \n",
    "    # Compute phi^r\n",
    "    #r = get_r(model, X_r,fun_r)\n",
    "    #phi_r = tf.reduce_mean(tf.square(r))\n",
    "    \n",
    "    # Initialize loss\n",
    "    loss = 0\n",
    "    \n",
    "    fun = [fun_u_0]\n",
    "    # Add phi^0 and phi^b to the loss x_data 边界值\n",
    "    for f in fun: \n",
    "        u_pred = get_r(model,f)\n",
    "        print(u_pred)\n",
    "        loss += tf.reduce_mean(tf.square(u_pred))\n",
    "\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad(model, X_r, X_data):\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "    # This tape is for derivatives with\n",
    "    # respect to trainable variables\n",
    "        tape.watch(model.trainable_variables)\n",
    "        loss = compute_loss(model, X_r, X_data)\n",
    "\n",
    "    g = tape.gradient(loss, model.trainable_variables)\n",
    "    del tape\n",
    "\n",
    "    return loss, g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model aka u_\\theta\n",
    "model = init_model()\n",
    "\n",
    "# We choose a piecewise decay of the learning rate, i.e., the\n",
    "# step size in the gradient descent type algorithm\n",
    "# the first 1000 steps use a learning rate of 0.01\n",
    "# from 1000 - 3000: learning rate = 0.001\n",
    "# from 3000 onwards: learning rate = 0.0005\n",
    "\n",
    "lr = tf.keras.optimizers.schedules.PiecewiseConstantDecay([1000,3000],[1e-2,1e-3,5e-4])\n",
    "\n",
    "# Choose the optimizer\n",
    "optim = tf.keras.optimizers.Adam(learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"stack:0\", shape=(1, 2), dtype=float32)\n",
      "Tensor(\"stack_1:0\", shape=(1, 2), dtype=float32)\n",
      "Tensor(\"stack:0\", shape=(1, 2), dtype=float32)\n",
      "Tensor(\"stack_1:0\", shape=(1, 2), dtype=float32)\n",
      "It 00000: loss = 6.20832622e-01\n",
      "It 00050: loss = 4.82026808e-04\n",
      "It 00100: loss = 5.72055660e-06\n",
      "It 00150: loss = 2.80710086e-08\n",
      "It 00200: loss = 9.13047415e-12\n",
      "It 00250: loss = 3.48610030e-13\n",
      "It 00300: loss = 1.67421632e-13\n",
      "It 00350: loss = 5.68434189e-14\n",
      "It 00400: loss = 8.88178420e-15\n",
      "It 00450: loss = 7.54951657e-15\n",
      "It 00500: loss = 4.44089210e-16\n",
      "It 00550: loss = 7.28306304e-14\n",
      "It 00600: loss = 3.01980663e-14\n",
      "It 00650: loss = 2.66897615e-13\n",
      "It 00700: loss = 2.31326069e-12\n",
      "It 00750: loss = 7.54951657e-15\n",
      "It 00800: loss = 8.74855743e-14\n",
      "It 00850: loss = 2.22044605e-15\n",
      "It 00900: loss = 1.15463195e-13\n",
      "It 00950: loss = 6.29274410e-13\n",
      "It 01000: loss = 2.66897615e-13\n",
      "It 01050: loss = 2.22044605e-15\n",
      "It 01100: loss = 8.88178420e-15\n",
      "It 01150: loss = 2.70894418e-14\n",
      "It 01200: loss = 2.22044605e-15\n",
      "It 01250: loss = 2.22044605e-15\n",
      "It 01300: loss = 2.22044605e-15\n",
      "It 01350: loss = 2.22044605e-15\n",
      "It 01400: loss = 2.22044605e-15\n",
      "It 01450: loss = 1.28785871e-14\n",
      "It 01500: loss = 1.77635684e-15\n",
      "It 01550: loss = 1.82076576e-14\n",
      "It 01600: loss = 9.81437154e-14\n",
      "It 01650: loss = 8.88178420e-15\n",
      "It 01700: loss = 8.88178420e-15\n",
      "It 01750: loss = 2.22044605e-15\n",
      "It 01800: loss = 2.22044605e-15\n",
      "It 01850: loss = 4.44089210e-16\n",
      "It 01900: loss = 1.28785871e-14\n",
      "It 01950: loss = 4.44089210e-16\n",
      "It 02000: loss = 4.44089210e-16\n",
      "It 02050: loss = 1.77635684e-15\n",
      "It 02100: loss = 1.77635684e-15\n",
      "It 02150: loss = 4.44089210e-16\n",
      "It 02200: loss = 4.44089210e-16\n",
      "It 02250: loss = 2.22044605e-15\n",
      "It 02300: loss = 4.44089210e-16\n",
      "It 02350: loss = 1.82076576e-14\n",
      "It 02400: loss = 7.54951657e-15\n",
      "It 02450: loss = 2.22044605e-15\n",
      "It 02500: loss = 2.22044605e-15\n",
      "It 02550: loss = 1.82076576e-14\n",
      "It 02600: loss = 2.22044605e-15\n",
      "It 02650: loss = 1.64313008e-14\n",
      "It 02700: loss = 2.22044605e-15\n",
      "It 02750: loss = 3.55271368e-15\n",
      "It 02800: loss = 4.44089210e-16\n",
      "It 02850: loss = 4.44089210e-16\n",
      "It 02900: loss = 2.22044605e-15\n",
      "It 02950: loss = 2.22044605e-15\n",
      "It 03000: loss = 9.81437154e-14\n",
      "It 03050: loss = 2.22044605e-15\n",
      "It 03100: loss = 8.88178420e-15\n",
      "It 03150: loss = 4.44089210e-16\n",
      "It 03200: loss = 1.64313008e-14\n",
      "It 03250: loss = 2.70894418e-14\n",
      "It 03300: loss = 8.88178420e-15\n",
      "It 03350: loss = 2.70894418e-14\n",
      "It 03400: loss = 1.64313008e-14\n",
      "It 03450: loss = 1.64313008e-14\n",
      "It 03500: loss = 7.54951657e-15\n",
      "It 03550: loss = 4.44089210e-16\n",
      "It 03600: loss = 4.44089210e-16\n",
      "It 03650: loss = 4.44089210e-16\n",
      "It 03700: loss = 2.22044605e-15\n",
      "It 03750: loss = 2.22044605e-15\n",
      "It 03800: loss = 7.54951657e-15\n",
      "It 03850: loss = 2.88657986e-14\n",
      "It 03900: loss = 6.43929354e-14\n",
      "It 03950: loss = 1.82076576e-14\n",
      "It 04000: loss = 2.22044605e-15\n",
      "It 04050: loss = 2.22044605e-15\n",
      "It 04100: loss = 1.64313008e-14\n",
      "It 04150: loss = 8.88178420e-15\n",
      "It 04200: loss = 8.88178420e-15\n",
      "It 04250: loss = 1.64313008e-14\n",
      "It 04300: loss = 2.22044605e-15\n",
      "It 04350: loss = 7.54951657e-15\n",
      "It 04400: loss = 2.22044605e-15\n",
      "It 04450: loss = 1.77635684e-14\n",
      "It 04500: loss = 2.88657986e-14\n",
      "It 04550: loss = 8.88178420e-14\n",
      "It 04600: loss = 1.77635684e-15\n",
      "It 04650: loss = 8.88178420e-15\n",
      "It 04700: loss = 1.77635684e-14\n",
      "It 04750: loss = 8.88178420e-15\n",
      "It 04800: loss = 1.88737914e-13\n",
      "It 04850: loss = 5.55111512e-14\n",
      "It 04900: loss = 2.88657986e-14\n",
      "It 04950: loss = 8.88178420e-15\n",
      "It 05000: loss = 9.81437154e-14\n",
      "\n",
      "Computation time: 2.1357100009918213 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "# Define one training step as a TensorFlow function to increase speed of training\n",
    "@tf.function\n",
    "def train_step():\n",
    "    # Compute current loss and gradient w.r.t. parameters\n",
    "    loss, grad_theta = get_grad(model, X_r, X_data)\n",
    "    \n",
    "    # Perform gradient descent step\n",
    "    optim.apply_gradients(zip(grad_theta, model.trainable_variables))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Number of training epochs\n",
    "N = 5000\n",
    "hist = []\n",
    "\n",
    "# Start timer\n",
    "t0 = time()\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    # Split t and x to compute partial derivatives\n",
    "\n",
    "    # Variables t and x are watched during tape\n",
    "    # to compute derivatives u_t and u_x\n",
    "    tape.watch(X_r)\n",
    "\n",
    "    # Determine residual \n",
    "    u = model(X_r)\n",
    "    # Compute gradient u_x within the GradientTape\n",
    "    # since we need second derivatives\n",
    "    u_x = tape.gradient(u, X_r)\n",
    "    del tape\n",
    "\n",
    "for i in range(N+1):\n",
    "    \n",
    "    loss = train_step()\n",
    "    \n",
    "    # Append current loss to hist\n",
    "    hist.append(loss.numpy())\n",
    "    \n",
    "    # Output current loss after 50 iterates\n",
    "    if i%50 == 0:\n",
    "        print('It {:05d}: loss = {:10.8e}'.format(i,loss))\n",
    "        \n",
    "# Print computation time\n",
    "print('\\nComputation time: {} seconds'.format(time()-t0))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5f9daefdf3bd6a1f70447da7dae243166419b6cefef564f1d2173f5c290c82bb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
